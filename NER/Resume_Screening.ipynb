{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee75e71d",
   "metadata": {},
   "source": [
    "# Resume Screening using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d2b8b",
   "metadata": {},
   "source": [
    "This project involves using spacy to perform entity recognition on 200 resumes and exploring different NLP tools for text analysis. The goal is to assist recruiters in quickly sifting through a large number of job applications. To aid hiring managers in determining whether to proceed to the interview stage, a skills match feature has been added, which uses a metric. Two datasets will be used, one containing resume texts and the other containing skills that will be used to create an entity ruler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85864b7a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428c89f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading/ Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "\n",
    " #nltk\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "#visualize\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "\n",
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1350cacc",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6213aebd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Resume/Resume.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11628/2060703160.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Resume/Resume.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m data = df.copy().iloc[\n\u001b[1;32m      4\u001b[0m     \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Resume/Resume.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Resume/Resume.csv\")\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "data = df.copy().iloc[\n",
    "    0:200,\n",
    "]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240b5e3",
   "metadata": {},
   "source": [
    "# Loading spaCy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "skill_pattern_path = \"jz_skill_patterns.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3f91f",
   "metadata": {},
   "source": [
    "# Entity Ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(skill_pattern_path)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f3186",
   "metadata": {},
   "source": [
    "# Skiils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    myset = []\n",
    "    subset = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"SKILL\":\n",
    "            subset.append(ent.text)\n",
    "    myset.append(subset)\n",
    "    return subset\n",
    "\n",
    "\n",
    "def unique_skills(x):\n",
    "    return list(set(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048321a9",
   "metadata": {},
   "source": [
    "# Resume Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(['stopwords','wordnet'])\n",
    "_ = nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "for i in range(data.shape[0]):\n",
    "    review = re.sub(\n",
    "        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n",
    "        \" \",\n",
    "        data[\"Resume_str\"].iloc[i],\n",
    "    )\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    lm = WordNetLemmatizer()\n",
    "    review = [\n",
    "        lm.lemmatize(word)\n",
    "        for word in review\n",
    "        if not word in set(stopwords.words(\"english\"))\n",
    "    ]\n",
    "    review = \" \".join(review)\n",
    "    clean.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e8200c",
   "metadata": {},
   "source": [
    "# Implementing the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab531602",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[\"Clean_Resume\"] = clean\n",
    "data[\"skills\"] = data[\"Clean_Resume\"].str.lower().apply(get_skills)\n",
    "data[\"skills\"] = data[\"skills\"].apply(unique_skills)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab74822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "fig = px.histogram(data, x=\"Category\", title=\"Distribution of Jobs Categories\", color=\"Category\",\n",
    "                   color_discrete_sequence=px.colors.qualitative.Pastel)\n",
    "\n",
    "fig.update_xaxes(categoryorder=\"total descending\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58899414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the fig. we can asses what skills a candidate needs to increase the probability of getting hired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_cat = data[\"Category\"].unique()\n",
    "Job_cat = np.append(Job_cat, \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968fec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to know the skills required for someone apply for a BUSINESS-DEVELOPMENT job\n",
    "\n",
    "Job_Category= 'BUSINESS-DEVELOPMENT'\n",
    "\n",
    "Total_skills = []\n",
    "if Job_Category != \"ALL\":\n",
    "    fltr = data[data[\"Category\"] == Job_Category][\"skills\"]\n",
    "    for x in fltr:\n",
    "        for i in x:\n",
    "            Total_skills.append(i)\n",
    "else:\n",
    "    fltr = data[\"skills\"]\n",
    "    for x in fltr:\n",
    "        for i in x:\n",
    "            Total_skills.append(i)\n",
    "\n",
    "fig = px.histogram(data, x=Total_skills, title=f\"{Job_Category} Distribution of Skills\",\n",
    "                   color=Total_skills,\n",
    "                   color_discrete_sequence=px.colors.qualitative.Pastel)\n",
    "\n",
    "fig.update_xaxes(categoryorder=\"total descending\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dff8ce",
   "metadata": {},
   "source": [
    "# Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ddfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nlp(data[\"Resume_str\"].iloc[0])\n",
    "displacy.render(sent, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a420cbd",
   "metadata": {},
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(sent[0:10], style=\"dep\", jupyter=True, options={\"distance\": 90})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651c3f8",
   "metadata": {},
   "source": [
    "# Custom Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39805a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = df.Category.unique()\n",
    "for a in patterns:\n",
    "    ruler.add_patterns([{\"label\": \"Job-Category\", \"pattern\": a}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d772e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options=[{\"ents\": \"Job-Category\", \"colors\": \"#ff3232\"},{\"ents\": \"SKILL\", \"colors\": \"#56c426\"}]\n",
    "colors = {\n",
    "    \"Job-Category\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "    \"SKILL\": \"linear-gradient(90deg, #9BE15D, #00E3AE)\",\n",
    "    \"ORG\": \"#ffd966\",\n",
    "    \"PERSON\": \"#e06666\",\n",
    "    \"GPE\": \"#9fc5e8\",\n",
    "    \"DATE\": \"#c27ba0\",\n",
    "    \"ORDINAL\": \"#674ea7\",\n",
    "    \"PRODUCT\": \"#f9cb9c\",\n",
    "}\n",
    "options = {\n",
    "    \"ents\": [\n",
    "        \"Job-Category\",\n",
    "        \"SKILL\",\n",
    "        \"ORG\",\n",
    "        \"PERSON\",\n",
    "        \"GPE\",\n",
    "        \"DATE\",\n",
    "        \"ORDINAL\",\n",
    "        \"PRODUCT\",\n",
    "    ],\n",
    "    \"colors\": colors,\n",
    "}\n",
    "sent = nlp(data[\"Resume_str\"].iloc[5])\n",
    "displacy.render(sent, style=\"ent\", jupyter=True, options=options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee6443",
   "metadata": {},
   "source": [
    "# Resume Anlaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking example of a random resume \n",
    "input_resume= \"Jonathan Andrews Data Scientist I am a certified data scientist professional, who loves building machine learning models and blogs about the latest AI technologies. I am currently testing AI Products at PEC-PITC, which later gets approved for human trials. jandrews@email.com +112345678 Phoenix, Arizona abidaliawan.me WORK EXPERIENCE Data Scientist Arizona Innovation and Testing Center - PEC 04/2021 - Present, Phoenix, Arizona Redesigned data of engineers that were mostly scattered and unavailable. Designed dashboard and data analysis report to help higher management make better decisions. Accessibility of key information has created a new culture of making data-driven decisions. Contact: Rameez Ali - rameezali@email.com Data Scientist Freelancing/Kaggle 11/2020 - Present, Phoenix, Arizona Engineered a healthcare system. Used machine learning to detect some of the common decisions. The project has paved the way for others to use new techniques to get better results. Participated in Kaggle machine learning competitions. Learned new techniques to get a better score and finally got to 1 percent rank. Researcher / Event Organizer CREDIT 02/2017 - 07/2017, Kuala Lumpur, Malaysia Marketing for newly build research lab. Organized technical events and successfully invited the multiple company's CEO for talks. Reduced the gap between industries and educational institutes. Research on new development in the IoT sector. Created research proposal for funding. Investigated the new communication protocol for IoT devices. Contact: Dr. Tan Chye Cheah - dr.chyecheah.t@apu.edu.my EDUCATION MSc in Technology Management Staffordshire University 11/2015 - 04/2017, Postgraduate with Distinction Challenges in Implementing IoT-enabled Smart cities in Malaysia. Bachelors Electrical Telecommunication Engineering COMSATS Institute of Information Technology, Phoenix 08/2010 - 01/2014, CGPA: 4.00 Networking Satellite communications Programming/ Matlab Telecommunication Engineering SKILLS Designing Leadership Media/Marketing R/Python SQL Tableau NLP Data Analysis Machine learning Deep learning Webapp/Cloud Feature Engineering Ensembling Time Series Technology Management ACHIEVEMENTS 98th Hungry Geese Simulation Competition (08/2021) 2nd in Covid-19 vaccinations around the world (07/2021) 8th in Automatic Speech Recognition in WOLOF (06/2021) Top 10 in WiDS Datathon. (03/2021) 40th / 622 in MagNet: Model the Geomagnetic Field Hosted by NOAA (02/2021) 18th in Rock, Paper, Scissors/Designing AI Agent Competition. (02/2021) PROJECTS Goodreads Profile Analysis WebApp (09/2021) Data Analysis Web Scraping XLM Interactive Visualization Contributed in orchest.io (08/2021) Testing and Debuging Technical Article Proposing new was to Improve ML pipelines World Vaccine Update System (06/2021) Used sqlite3 for database Automated system for daily update the Kaggle DB and Analysis Interactive dashboard mRNA-Vaccine-Degradation-Prediction (06/2021) Explore our dataset and then preprocessed sequence, structure, and predicted loop type features Train deep learning GRU model Trip Advisor Data Analysis/ML (04/2021) Preprocessing Data, Exploratory Data analysis, Word clouds. Feature Engineering, Text processing. BiLSTM Model for predicting rating, evaluation, model performance. Jane Street Market Prediction (03/2021) EDA, Feature Engineering, experimenting with hyperparameters. Ensembling: Resnet, NN Embeddings, TF Simple NN model. Using simple MLP pytorch model. Achievements/Tasks Achievements/Tasks Achievements/Tasks Thesis Courses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a549808",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = nlp(input_resume)\n",
    "displacy.render(sent2, style=\"ent\", jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6be761",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a458a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_skills = \"Database, SQL, Machine Learning\" # example of input skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_skills = input_skills.lower().split(\",\")\n",
    "resume_skills = unique_skills(get_skills(input_resume.lower()))\n",
    "score = 0\n",
    "for x in req_skills:\n",
    "    if x in resume_skills:\n",
    "        score += 1\n",
    "req_skills_len = len(req_skills)\n",
    "match = round(score / req_skills_len * 100, 1)\n",
    "\n",
    "print(f\"The current Resume is {match}% matched to your requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e2910",
   "metadata": {},
   "source": [
    "We can also see the skills mentioned in the input resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4758dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resume_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed0752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
